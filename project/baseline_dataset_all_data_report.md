# Report with RandomForest + Cosine similarities and all data

## Сравнение: маленький vs большой сбалансированный датасет

| | маленький | большой |
|---|---|---|
| **Данные** | 1 652 (по 413 на класс) | 56 584 (по 14 146 на класс) |
| **Test accuracy** | 0.2915 | 0.2937 |
| **Test macro-F1** | 0.29 | 0.29 |
| **match F1** | 0.19 | 0.29 |
| **no_match F1** | 0.45 | 0.36 |
| **related F1** | 0.31 | 0.26 |
| **weak_match F1** | 0.20 | 0.25 |

## Что показывает эксперимент

1. **Увеличение данных не улучшило macro-F1.** Это ключевой результат. На маленьком датасете macro-F1 = 0.29, на большом — тоже 0.29. Это означает, что модель **уже достигла своего потолка** — больше данных не даст лучшее качество при текущей комбинации модели и признаков.

2. **Метрики стали стабильнее.** На большом датасете все классы получили более ровные F1 (0.25–0.36). Это ожидаемо: больше данных — меньше случайности в оценках, но средний уровень качества не изменился.

## Что нужно для качественного скачка

Эксперимент показал, что задача требует **смены подхода**, а не увеличения данных. Наиболее перспективное направление — **Deep Learning с эмбеддингами**:

- вместо сжатия 384-мерных эмбеддингов в одно число (cosine similarity), подать сырые эмбеддинги (rfq + supplier + multilingual) в нейронную сеть, которая сама найдёт нелинейные комбинации признаков. В отличие от RandomForest, нейросеть может обучаться на всех 384 измерениях одновременно.

Именно большой датасет (56k) делает Deep Learning реальным вариантом — на 1 652 примерах нейросеть бы переобучилась, а на 56k уже есть достаточно данных для обучения.

![Visualizations](all_data_RF.png)
